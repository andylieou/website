{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67f4a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc515fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 input                  response\n",
      "0    Milo I see a dog?                     grrrr\n",
      "1  how's your day been          woof woof (good)\n",
      "2             ready???  jump! jump! (I am ready)\n",
      "3          ready ready  jump! jump! (I am ready)\n",
      "4       milo whos here                     grrrr\n",
      "5                   hi              bark (hello)\n",
      "6                Adios            woof (goodbye)\n",
      "7              Attack!                     grrrr\n",
      "8      milo go attack!                     grrrr\n",
      "9        Ready to eat?  jump! jump! (I am ready)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('milo.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45671e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 1.5690 | Val Loss: 1.3629 | Val Acc: 0.4483\n",
      "Epoch 2/10 | Train Loss: 1.1651 | Val Loss: 0.9478 | Val Acc: 0.8276\n",
      "Epoch 3/10 | Train Loss: 0.8342 | Val Loss: 0.6496 | Val Acc: 0.8966\n",
      "Epoch 4/10 | Train Loss: 0.5484 | Val Loss: 0.3916 | Val Acc: 1.0000\n",
      "Epoch 5/10 | Train Loss: 0.3312 | Val Loss: 0.2137 | Val Acc: 1.0000\n",
      "Epoch 6/10 | Train Loss: 0.1993 | Val Loss: 0.1240 | Val Acc: 1.0000\n",
      "Epoch 7/10 | Train Loss: 0.1267 | Val Loss: 0.0795 | Val Acc: 1.0000\n",
      "Epoch 8/10 | Train Loss: 0.0858 | Val Loss: 0.0586 | Val Acc: 1.0000\n",
      "Epoch 9/10 | Train Loss: 0.0664 | Val Loss: 0.0443 | Val Acc: 1.0000\n",
      "Epoch 10/10 | Train Loss: 0.0519 | Val Loss: 0.0361 | Val Acc: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MiloDataset(Dataset):\n",
    "    def __init__(self, texts, responses, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.responses = responses\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # assume responses are class indices (like 0, 1, 2, etc.)\n",
    "        label = torch.tensor(self.responses[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['response'].unique())}\n",
    "df['label'] = df['response'].map(label_mapping)\n",
    "\n",
    "full_dataset = MiloDataset(df['input'].tolist(), df['label'].tolist(), tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deb414e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milo says: jump! jump! (I am ready)\n"
     ]
    }
   ],
   "source": [
    "def predict_single_input(model, tokenizer, text, label_mapping, device):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    predicted_response = idx_to_label[prediction.item()]\n",
    "    return predicted_response\n",
    "\n",
    "your_input = \"are you ready to go outside?\"\n",
    "response = predict_single_input(model, tokenizer, your_input, label_mapping, device)\n",
    "print(\"Milo says:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs342",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
