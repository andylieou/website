{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67f4a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fc515fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    input                  response\n",
      "0            Good morning              bark (hello)\n",
      "1               Milo Sit!                      sits\n",
      "2                   Milo?                    pounce\n",
      "3    milo are you hungry?    bark (yes i want food)\n",
      "4    milo are you hungry?    bark (yes i want food)\n",
      "5    Milo is the best boy                 wags tail\n",
      "6           is milo ready  jump! jump! (I am ready)\n",
      "7   milo eat those people                     grrrr\n",
      "8  milo where is your paw                       paw\n",
      "9   are you ready to eat?  jump! jump! (I am ready)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('milo.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45671e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 2.4535 | Val Loss: 1.8965 | Val Acc: 0.6909\n",
      "Epoch 2/10 | Train Loss: 1.5953 | Val Loss: 1.0674 | Val Acc: 0.9636\n",
      "Epoch 3/10 | Train Loss: 0.7986 | Val Loss: 0.4695 | Val Acc: 0.9727\n",
      "Epoch 4/10 | Train Loss: 0.3858 | Val Loss: 0.2247 | Val Acc: 1.0000\n",
      "Epoch 5/10 | Train Loss: 0.2183 | Val Loss: 0.1306 | Val Acc: 1.0000\n",
      "Epoch 6/10 | Train Loss: 0.1444 | Val Loss: 0.0870 | Val Acc: 1.0000\n",
      "Epoch 7/10 | Train Loss: 0.1057 | Val Loss: 0.0742 | Val Acc: 0.9909\n",
      "Epoch 8/10 | Train Loss: 0.0848 | Val Loss: 0.0490 | Val Acc: 1.0000\n",
      "Epoch 9/10 | Train Loss: 0.0648 | Val Loss: 0.0397 | Val Acc: 1.0000\n",
      "Epoch 10/10 | Train Loss: 0.0624 | Val Loss: 0.0346 | Val Acc: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MiloDataset(Dataset):\n",
    "    def __init__(self, texts, responses, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.responses = responses\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # assume responses are class indices (like 0, 1, 2, etc.)\n",
    "        label = torch.tensor(self.responses[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['response'].unique())}\n",
    "df['label'] = df['response'].map(label_mapping)\n",
    "\n",
    "full_dataset = MiloDataset(df['input'].tolist(), df['label'].tolist(), tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def eval_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "num_epochs = 6\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "665f641c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('milo_model/tokenizer_config.json',\n",
       " 'milo_model/special_tokens_map.json',\n",
       " 'milo_model/vocab.txt',\n",
       " 'milo_model/added_tokens.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"milo_model\")\n",
    "tokenizer.save_pretrained(\"milo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deb414e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milo says: grrrr\n"
     ]
    }
   ],
   "source": [
    "def predict_single_input(model, tokenizer, text, label_mapping, device):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
    "    predicted_response = idx_to_label[prediction.item()]\n",
    "    return predicted_response\n",
    "\n",
    "your_input = \"bark\"\n",
    "response = predict_single_input(model, tokenizer, your_input, label_mapping, device)\n",
    "print(\"Milo says:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs342",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
